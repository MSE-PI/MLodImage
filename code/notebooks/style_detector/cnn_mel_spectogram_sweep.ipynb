{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Music Style Detector : CNN with Mel Spectogram\n",
    "### Sweeps\n",
    "\n",
    "This notebook contains code to perform sweep over hyperparameters for CNN with Mel Spectogram.\n",
    "\n",
    "### 1. Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# CONSTANTS\n",
    "DATA_DIR = os.path.abspath(os.path.join(os.getcwd(), '../..', 'models', 'genre_detector', 'data'))\n",
    "AUDIO_DIR = os.path.join(DATA_DIR, 'raw', 'audio')\n",
    "TRAIN_DF = pd.read_csv(os.path.join(DATA_DIR, 'prepared', 'train_genres.csv'))\n",
    "NB_CLASSES = len(TRAIN_DF['genre_id'].unique())\n",
    "ID_TO_LABEL = TRAIN_DF.set_index('genre_id')['genre_label'].to_dict()\n",
    "\n",
    "# SWEEP CONFIG\n",
    "sweep_config = {\n",
    "    'name': 'CNN Mel Spectogram Sweep',\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'audio_duration': {\n",
    "            'values': [10000, 20000, 30000]\n",
    "        },\n",
    "        'sample_rate': {\n",
    "            'values': [44100, 48000]\n",
    "        },\n",
    "        'n_channels': {\n",
    "            'values': [1, 2]\n",
    "        },\n",
    "        'time_shift': {\n",
    "            'values': [0.2, 0.3, 0.4]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'lr': {\n",
    "            'values': [0.001, 0.0001]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0, 0.2, 0.5]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: w55w5saa\n",
      "Sweep URL: https://wandb.ai/mlodimage/genre-detector_sweep/sweeps/w55w5saa\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='genre-detector_sweep', entity='mlodimage')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtils():\n",
    "    \"\"\"\n",
    "    Utility class for audio processing.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def open(audio_file: str):\n",
    "        \"\"\"\n",
    "        Load an audio file. Return the signal as a tensor and the sample rate.\n",
    "        :param audio_file : Path to the audio file.\n",
    "        :type audio_file : str\n",
    "        :return: signal as a tensor and the sample rate\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = torchaudio.load(audio_file)\n",
    "        return signal, sample_rate\n",
    "    \n",
    "    @staticmethod\n",
    "    def rechannel(audio, new_channel):\n",
    "        \"\"\"\n",
    "        Convert a given audio to the specified number of channels.\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param new_channel: the target number of channels\n",
    "        :type new_channel: int\n",
    "        :return: the audio with the target number of channels\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "\n",
    "        if signal.shape[0] == new_channel:\n",
    "            # nothing to do as the signal already has the target number of channels\n",
    "            return audio\n",
    "        if new_channel == 1:\n",
    "            # convert to mono by selecting only the first channel\n",
    "            signal = signal[:1, :]\n",
    "        else:\n",
    "            # convert to stereo by duplicating the first channel\n",
    "            signal = torch.cat([signal, signal])\n",
    "        return signal, sample_rate\n",
    "    \n",
    "    @staticmethod\n",
    "    def resample(audio, new_sample_rate):\n",
    "        \"\"\"\n",
    "        Change the sample rate of the audio signal.\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param new_sample_rate: the target sample rate\n",
    "        :type new_sample_rate: int\n",
    "        :return: the audio with the target sample rate\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "        if sample_rate == new_sample_rate:\n",
    "            # nothing to do\n",
    "            return audio\n",
    "        resample = torchaudio.transforms.Resample(sample_rate, new_sample_rate)\n",
    "        signal = resample(signal)\n",
    "        return signal, new_sample_rate\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_truncate(audio, length):\n",
    "        \"\"\"\n",
    "        Pad or truncate an audio signal to a fixed length (in ms).\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param length: the target length in ms\n",
    "        :type length: int\n",
    "        :return: the audio with the target length\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "        max_length = sample_rate//1000 * length\n",
    "\n",
    "        if signal.shape[1] > max_length:\n",
    "            signal = signal[:, :max_length]\n",
    "        elif signal.shape[1] < max_length:\n",
    "            padding = max_length - signal.shape[1]\n",
    "            signal = F.pad(signal, (0, padding))\n",
    "        return signal, sample_rate\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(audio, shift_limit):\n",
    "        \"\"\"\n",
    "        Shift the signal to the left or right by some percent. Values at the end\n",
    "        are 'wrapped around' to the start of the transformed signal.\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param shift_limit: the maximum shift to apply (in percent)\n",
    "        :type shift_limit: int\n",
    "        :return: the shifted audio\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "        _, signal_length = signal.shape\n",
    "        shift_amount = int(random.random() * shift_limit * signal_length)\n",
    "        return (signal.roll(shift_amount), sample_rate)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mel_spectrogram(audio, n_mels=64, n_fft=2048, hop_length=None):\n",
    "        \"\"\"\n",
    "        Create the mel spectogram for the given audio signal.\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param n_mels: the number of mel filterbanks\n",
    "        :type n_mels: int\n",
    "        :param n_fft: the size of the FFT\n",
    "        :type n_fft: int\n",
    "        :param hop_length: the length of hop between STFT windows\n",
    "        :type hop_length: int\n",
    "        :return: the mel spectogram\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "        \n",
    "        mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels\n",
    "        )(signal)\n",
    "\n",
    "        # convert to decibels\n",
    "        mel_spectrogram = torchaudio.transforms.AmplitudeToDB(top_db=80)(mel_spectrogram)\n",
    "\n",
    "        return mel_spectrogram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the FMA dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, audio_dir, config):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        :param df: the dataframe containing the audio files ids and their genre label\n",
    "        :type df: pandas.DataFrame\n",
    "        :param audio_dir: the directory containing the audio files\n",
    "        :type audio_dir: str\n",
    "        \"\"\"\n",
    "        self.fma_df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.config = config\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the length of the dataset.\n",
    "        :return: the length of the dataset\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return len(self.fma_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the idx-th sample of the dataset.\n",
    "        :param idx: the index of the sample\n",
    "        :type idx: int\n",
    "        :return: the idx-th sample of the dataset and its genre label\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\" \n",
    "        audio_file_path = os.path.join(self.audio_dir, str(self.fma_df.iloc[idx]['filename']))\n",
    "        # get the genre class id\n",
    "        genre_id = self.fma_df.iloc[idx]['genre_id']\n",
    "\n",
    "        # load the audio file and apply the preprocessing\n",
    "        audio = AudioUtils.open(audio_file_path)\n",
    "        audio = AudioUtils.rechannel(audio, self.config.n_channels)\n",
    "        audio = AudioUtils.resample(audio, self.config.sample_rate)\n",
    "        audio = AudioUtils.pad_truncate(audio, self.config.audio_duration)\n",
    "        audio = AudioUtils.time_shift(audio, self.config.time_shift)\n",
    "        mel_spectrogram = AudioUtils.mel_spectrogram(audio)\n",
    "\n",
    "        return (mel_spectrogram, genre_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config):\n",
    "    # load the data\n",
    "    full_dataset = GenreDataset(TRAIN_DF, AUDIO_DIR, config)\n",
    "\n",
    "    # random split\n",
    "    nb_samples = len(full_dataset)\n",
    "    nb_train_samples = int(nb_samples * 0.8)\n",
    "    nb_val_samples = nb_samples - nb_train_samples\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [nb_train_samples, nb_val_samples])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False,  num_workers=8)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Audio classification model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, dropout, lr):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        :param nb_channels: the number of channels in the input data\n",
    "        :param nb_classes: the number of classes\n",
    "        :type nb_classes: int\n",
    "        \"\"\"\n",
    "        super(AudioCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.drop1 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.drop2 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.drop3 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.drop4 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.drop5 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight, a=0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.linear = nn.Linear(in_features=128, out_features=NB_CLASSES, bias=True)\n",
    "\n",
    "        # loss function\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        # optimizer parameters\n",
    "        self.lr = lr\n",
    "\n",
    "        # save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        :param x: the input\n",
    "        :type x: torch.Tensor\n",
    "        :return: the output\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        x = self.drop1(self.bn1(self.relu1(self.conv1(x))))\n",
    "        x = self.drop2(self.bn2(self.relu2(self.conv2(x))))\n",
    "        x = self.drop3(self.bn3(self.relu3(self.conv3(x))))\n",
    "        x = self.drop4(self.bn4(self.relu4(self.conv4(x))))\n",
    "        x = self.drop5(self.bn5(self.relu5(self.conv5(x))))\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "        :param batch: the batch\n",
    "        :type batch: Tuple[torch.Tensor, torch.Tensor]\n",
    "        :param batch_idx: the batch index\n",
    "        :type batch_idx: int\n",
    "        :return: the loss\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "        :param batch: the batch\n",
    "        :type batch: Tuple[torch.Tensor, torch.Tensor]\n",
    "        :param batch_idx: the batch index\n",
    "        :type batch_idx: int\n",
    "        :return: the loss\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step.\n",
    "        :param batch: the batch\n",
    "        :type batch: Tuple[torch.Tensor, torch.Tensor]\n",
    "        :param batch_idx: the batch index\n",
    "        :type batch_idx: int\n",
    "        :return: the loss\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizers.\n",
    "        :return: the optimizer\n",
    "        :rtype: torch.optim.Optimizer\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def _get_preds_loss_accuracy(self, batch):\n",
    "        \"\"\"\n",
    "        Get predictions, loss and accuracy.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        loss = self.loss(logits, y)\n",
    "        acc = accuracy(preds, y, 'multiclass', num_classes=NB_CLASSES)\n",
    "        return preds, loss, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # data\n",
    "        train_loader, val_loader = build_dataset(config)\n",
    "        # model\n",
    "        model = AudioCNN(n_channels=config.n_channels, dropout=config.dropout, lr=config.lr)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=40,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=5)],\n",
    "            logger=WandbLogger())\n",
    "        \n",
    "        trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=40)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
