{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Music Style Detector : CNN with Mel Spectogram\n",
    "### Sweeps\n",
    "\n",
    "This notebook contains code to perform sweep over hyperparameters for CNN with Mel Spectogram.\n",
    "\n",
    "### 1. Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# CONSTANTS\n",
    "DATA_DIR = os.path.abspath(os.path.join(os.getcwd(), '../..', 'models', 'genre_detector', 'data'))\n",
    "AUDIO_DIR = os.path.join(DATA_DIR, 'raw', 'audio')\n",
    "TRAIN_DF = pd.read_csv(os.path.join(DATA_DIR, 'prepared', 'train_genres.csv'))\n",
    "NB_CLASSES = len(TRAIN_DF['genre_id'].unique())\n",
    "ID_TO_LABEL = TRAIN_DF.set_index('genre_id')['genre_label'].to_dict()\n",
    "\n",
    "# SWEEP CONFIG\n",
    "sweep_config = {\n",
    "    'name': 'CNN Mel Spectogram Sweep',\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'audio_duration': {\n",
    "            'values': [10000, 20000, 30000]\n",
    "        },\n",
    "        'sample_rate': {\n",
    "            'values': [44100, 48000]\n",
    "        },\n",
    "        'n_channels': {\n",
    "            'values': [1, 2]\n",
    "        },\n",
    "        'time_shift': {\n",
    "            'values': [0.2, 0.3, 0.4]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'lr': {\n",
    "            'values': [0.001, 0.0001]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0, 0.2, 0.5]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8j0jr9ql\n",
      "Sweep URL: https://wandb.ai/mlodimage/genre-detector_sweep/sweeps/8j0jr9ql\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='genre-detector_sweep', entity='mlodimage')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtils():\n",
    "    \"\"\"\n",
    "    Utility class for audio processing.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def open(audio_file: str):\n",
    "        \"\"\"\n",
    "        Load an audio file. Return the signal as a tensor and the sample rate.\n",
    "        :param audio_file : Path to the audio file.\n",
    "        :type audio_file : str\n",
    "        :return: signal as a tensor and the sample rate\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = torchaudio.load(audio_file)\n",
    "        return signal, sample_rate\n",
    "    \n",
    "    @staticmethod\n",
    "    def rechannel(audio, new_channel):\n",
    "        \"\"\"\n",
    "        Convert a given audio to the specified number of channels.\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param new_channel: the target number of channels\n",
    "        :type new_channel: int\n",
    "        :return: the audio with the target number of channels\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "\n",
    "        if signal.shape[0] == new_channel:\n",
    "            # nothing to do as the signal already has the target number of channels\n",
    "            return audio\n",
    "        if new_channel == 1:\n",
    "            # convert to mono by selecting only the first channel\n",
    "            signal = signal[:1, :]\n",
    "        else:\n",
    "            # convert to stereo by duplicating the first channel\n",
    "            signal = torch.cat([signal, signal])\n",
    "        return signal, sample_rate\n",
    "    \n",
    "    @staticmethod\n",
    "    def resample(audio, new_sample_rate):\n",
    "        \"\"\"\n",
    "        Change the sample rate of the audio signal.\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param new_sample_rate: the target sample rate\n",
    "        :type new_sample_rate: int\n",
    "        :return: the audio with the target sample rate\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "        if sample_rate == new_sample_rate:\n",
    "            # nothing to do\n",
    "            return audio\n",
    "        resample = torchaudio.transforms.Resample(sample_rate, new_sample_rate)\n",
    "        signal = resample(signal)\n",
    "        return signal, new_sample_rate\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_truncate(audio, length):\n",
    "        \"\"\"\n",
    "        Pad or truncate an audio signal to a fixed length (in ms).\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param length: the target length in ms\n",
    "        :type length: int\n",
    "        :return: the audio with the target length\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "        max_length = sample_rate//1000 * length\n",
    "\n",
    "        if signal.shape[1] > max_length:\n",
    "            signal = signal[:, :max_length]\n",
    "        elif signal.shape[1] < max_length:\n",
    "            padding = max_length - signal.shape[1]\n",
    "            signal = F.pad(signal, (0, padding))\n",
    "        return signal, sample_rate\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(audio, shift_limit):\n",
    "        \"\"\"\n",
    "        Shift the signal to the left or right by some percent. Values at the end\n",
    "        are 'wrapped around' to the start of the transformed signal.\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param shift_limit: the maximum shift to apply (in percent)\n",
    "        :type shift_limit: int\n",
    "        :return: the shifted audio\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "        _, signal_length = signal.shape\n",
    "        shift_amount = int(random.random() * shift_limit * signal_length)\n",
    "        return (signal.roll(shift_amount), sample_rate)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mel_spectrogram(audio, n_mels=64, n_fft=2048, hop_length=None):\n",
    "        \"\"\"\n",
    "        Create the mel spectogram for the given audio signal.\n",
    "        :param audio: the audio, composed of the signal and the sample rate\n",
    "        :type audio: Tuple[torch.Tensor, int]\n",
    "        :param n_mels: the number of mel filterbanks\n",
    "        :type n_mels: int\n",
    "        :param n_fft: the size of the FFT\n",
    "        :type n_fft: int\n",
    "        :param hop_length: the length of hop between STFT windows\n",
    "        :type hop_length: int\n",
    "        :return: the mel spectogram\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        signal, sample_rate = audio\n",
    "        \n",
    "        mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels\n",
    "        )(signal)\n",
    "\n",
    "        # convert to decibels\n",
    "        mel_spectrogram = torchaudio.transforms.AmplitudeToDB(top_db=80)(mel_spectrogram)\n",
    "\n",
    "        return mel_spectrogram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the FMA dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, audio_dir, config):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        :param df: the dataframe containing the audio files ids and their genre label\n",
    "        :type df: pandas.DataFrame\n",
    "        :param audio_dir: the directory containing the audio files\n",
    "        :type audio_dir: str\n",
    "        \"\"\"\n",
    "        self.fma_df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.config = config\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the length of the dataset.\n",
    "        :return: the length of the dataset\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return len(self.fma_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the idx-th sample of the dataset.\n",
    "        :param idx: the index of the sample\n",
    "        :type idx: int\n",
    "        :return: the idx-th sample of the dataset and its genre label\n",
    "        :rtype: Tuple[torch.Tensor, int]\n",
    "        \"\"\" \n",
    "        audio_file_path = os.path.join(self.audio_dir, str(self.fma_df.iloc[idx]['filename']))\n",
    "        # get the genre class id\n",
    "        genre_id = self.fma_df.iloc[idx]['genre_id']\n",
    "\n",
    "        # load the audio file and apply the preprocessing\n",
    "        audio = AudioUtils.open(audio_file_path)\n",
    "        audio = AudioUtils.rechannel(audio, self.config.n_channels)\n",
    "        audio = AudioUtils.resample(audio, self.config.sample_rate)\n",
    "        audio = AudioUtils.pad_truncate(audio, self.config.audio_duration)\n",
    "        audio = AudioUtils.time_shift(audio, self.config.time_shift)\n",
    "        mel_spectrogram = AudioUtils.mel_spectrogram(audio)\n",
    "\n",
    "        return (mel_spectrogram, genre_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config):\n",
    "    # load the data\n",
    "    full_dataset = GenreDataset(TRAIN_DF, AUDIO_DIR, config)\n",
    "\n",
    "    # random split\n",
    "    nb_samples = len(full_dataset)\n",
    "    nb_train_samples = int(nb_samples * 0.8)\n",
    "    nb_val_samples = nb_samples - nb_train_samples\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [nb_train_samples, nb_val_samples])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=20, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False,  num_workers=20)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Audio classification model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, dropout, lr):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        :param nb_channels: the number of channels in the input data\n",
    "        :param nb_classes: the number of classes\n",
    "        :type nb_classes: int\n",
    "        \"\"\"\n",
    "        super(AudioCNN, self).__init__()\n",
    "\n",
    "        print(n_channels)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.drop1 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.drop2 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.drop3 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.drop4 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.drop5 = nn.Dropout2d(p=dropout)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight, a=0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.linear = nn.Linear(in_features=128, out_features=NB_CLASSES, bias=True)\n",
    "\n",
    "        # loss function\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        # optimizer parameters\n",
    "        self.lr = lr\n",
    "\n",
    "        # save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        :param x: the input\n",
    "        :type x: torch.Tensor\n",
    "        :return: the output\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        x = self.drop1(self.bn1(self.relu1(self.conv1(x))))\n",
    "        x = self.drop2(self.bn2(self.relu2(self.conv2(x))))\n",
    "        x = self.drop3(self.bn3(self.relu3(self.conv3(x))))\n",
    "        x = self.drop4(self.bn4(self.relu4(self.conv4(x))))\n",
    "        x = self.drop5(self.bn5(self.relu5(self.conv5(x))))\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "        :param batch: the batch\n",
    "        :type batch: Tuple[torch.Tensor, torch.Tensor]\n",
    "        :param batch_idx: the batch index\n",
    "        :type batch_idx: int\n",
    "        :return: the loss\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "        :param batch: the batch\n",
    "        :type batch: Tuple[torch.Tensor, torch.Tensor]\n",
    "        :param batch_idx: the batch index\n",
    "        :type batch_idx: int\n",
    "        :return: the loss\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step.\n",
    "        :param batch: the batch\n",
    "        :type batch: Tuple[torch.Tensor, torch.Tensor]\n",
    "        :param batch_idx: the batch index\n",
    "        :type batch_idx: int\n",
    "        :return: the loss\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizers.\n",
    "        :return: the optimizer\n",
    "        :rtype: torch.optim.Optimizer\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def _get_preds_loss_accuracy(self, batch):\n",
    "        \"\"\"\n",
    "        Get predictions, loss and accuracy.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        loss = self.loss(logits, y)\n",
    "        acc = accuracy(preds, y, 'multiclass', num_classes=NB_CLASSES)\n",
    "        return preds, loss, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # data\n",
    "        print('Done')\n",
    "        train_loader, val_loader = build_dataset(config)\n",
    "        print('Done')\n",
    "        # model\n",
    "        model = AudioCNN(n_channels=config.n_channels, dropout=config.dropout, lr=config.lr)\n",
    "        print('Done creating model')\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=2,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=5)],\n",
    "            logger=WandbLogger())\n",
    "        \n",
    "        trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i71lwgaw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taudio_duration: 20000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_channels: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsample_rate: 44100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_shift: 0.4\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/benja/Projects/MLodImage/code/notebooks/style_detector/wandb/run-20230607_120306-i71lwgaw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlodimage/genre-detector_sweep/runs/i71lwgaw' target=\"_blank\">kind-sweep-1</a></strong> to <a href='https://wandb.ai/mlodimage/genre-detector_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mlodimage/genre-detector_sweep/sweeps/8j0jr9ql' target=\"_blank\">https://wandb.ai/mlodimage/genre-detector_sweep/sweeps/8j0jr9ql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlodimage/genre-detector_sweep' target=\"_blank\">https://wandb.ai/mlodimage/genre-detector_sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mlodimage/genre-detector_sweep/sweeps/8j0jr9ql' target=\"_blank\">https://wandb.ai/mlodimage/genre-detector_sweep/sweeps/8j0jr9ql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlodimage/genre-detector_sweep/runs/i71lwgaw' target=\"_blank\">https://wandb.ai/mlodimage/genre-detector_sweep/runs/i71lwgaw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ZMQDisplayPublisher' object has no attribute '_orig_publish'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb\u001b[39m.\u001b[39magent(sweep_id, train, count\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m wandb\u001b[39m.\u001b[39;49mfinish()\n",
      "File \u001b[0;32m~/anaconda3/envs/pi_mlodimage/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:3705\u001b[0m, in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   3695\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[1;32m   3696\u001b[0m \n\u001b[1;32m   3697\u001b[0m \u001b[39mThis is used when creating multiple runs in the same process.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3702\u001b[0m \u001b[39m    quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   3703\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3704\u001b[0m \u001b[39mif\u001b[39;00m wandb\u001b[39m.\u001b[39mrun:\n\u001b[0;32m-> 3705\u001b[0m     wandb\u001b[39m.\u001b[39;49mrun\u001b[39m.\u001b[39;49mfinish(exit_code\u001b[39m=\u001b[39;49mexit_code, quiet\u001b[39m=\u001b[39;49mquiet)\n",
      "File \u001b[0;32m~/anaconda3/envs/pi_mlodimage/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:394\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         wandb\u001b[39m.\u001b[39mtermwarn(message, repeat\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    392\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mDummy()\n\u001b[0;32m--> 394\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pi_mlodimage/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:335\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    334\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_is_attaching \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 335\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pi_mlodimage/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1884\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1870\u001b[0m \u001b[39m@_run_decorator\u001b[39m\u001b[39m.\u001b[39m_noop\n\u001b[1;32m   1871\u001b[0m \u001b[39m@_run_decorator\u001b[39m\u001b[39m.\u001b[39m_attach\n\u001b[1;32m   1872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish\u001b[39m(\n\u001b[1;32m   1873\u001b[0m     \u001b[39mself\u001b[39m, exit_code: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, quiet: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1874\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1875\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[1;32m   1876\u001b[0m \n\u001b[1;32m   1877\u001b[0m \u001b[39m    This is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1882\u001b[0m \u001b[39m        quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   1883\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_finish(exit_code, quiet)\n",
      "File \u001b[0;32m~/anaconda3/envs/pi_mlodimage/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1897\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1895\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown_hooks:\n\u001b[1;32m   1896\u001b[0m     \u001b[39mif\u001b[39;00m hook\u001b[39m.\u001b[39mstage \u001b[39m==\u001b[39m TeardownStage\u001b[39m.\u001b[39mEARLY:\n\u001b[0;32m-> 1897\u001b[0m         hook\u001b[39m.\u001b[39;49mcall()\n\u001b[1;32m   1899\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_atexit_cleanup(exit_code\u001b[39m=\u001b[39mexit_code)\n\u001b[1;32m   1900\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wl \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wl\u001b[39m.\u001b[39m_global_run_stack) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pi_mlodimage/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:443\u001b[0m, in \u001b[0;36m_WandbInit._jupyter_teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_pause_backend\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m hook\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m:\n\u001b[1;32m    442\u001b[0m         ipython\u001b[39m.\u001b[39mevents\u001b[39m.\u001b[39munregister(\u001b[39m\"\u001b[39m\u001b[39mpost_run_cell\u001b[39m\u001b[39m\"\u001b[39m, hook)\n\u001b[0;32m--> 443\u001b[0m ipython\u001b[39m.\u001b[39mdisplay_pub\u001b[39m.\u001b[39mpublish \u001b[39m=\u001b[39m ipython\u001b[39m.\u001b[39;49mdisplay_pub\u001b[39m.\u001b[39;49m_orig_publish\n\u001b[1;32m    444\u001b[0m \u001b[39mdel\u001b[39;00m ipython\u001b[39m.\u001b[39mdisplay_pub\u001b[39m.\u001b[39m_orig_publish\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kind-sweep-1</strong> at: <a href='https://wandb.ai/mlodimage/genre-detector_sweep/runs/i71lwgaw' target=\"_blank\">https://wandb.ai/mlodimage/genre-detector_sweep/runs/i71lwgaw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230607_120306-i71lwgaw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=3)\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
